---
sidebar_position: 1
---

# Module 4: Vision-Language-Action (VLA)

## Overview

Welcome to Module 4 of the Physical AI & Humanoid Robotics course. This module focuses on Vision-Language-Action (VLA) systems that integrate voice commands, LLM-based task planning, and autonomous humanoid capabilities. This capstone module combines all previous learning into a comprehensive autonomous humanoid system.

## Learning Objectives

By the end of this module, you will be able to:
- Implement voice command processing with Whisper
- Design LLM-based task planning systems for robotics
- Integrate vision-language models with robotic control
- Create autonomous humanoid behaviors
- Combine all previous modules into a unified system

## Module Structure

This module is divided into three main chapters:
1. [Voice Commands](./voice-commands) - Processing and interpreting voice input
2. [LLM Task Planning](./llm-planning) - AI-driven task decomposition and planning
3. [Capstone Autonomous Humanoid](./capstone-autonomous) - Complete autonomous humanoid implementation

## Prerequisites

This module assumes you have completed all previous modules. Familiarity with:
- All ROS 2 concepts and Python AI agents
- Simulation environments and sensor systems
- Isaac Sim and Isaac ROS
- Nav2 path planning and navigation

## Target Audience

AI and Robotics students who want to implement advanced AI control systems that respond to voice commands and use LLM-based task planning for autonomous humanoid operation.